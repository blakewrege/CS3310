import scala.math.random
import org.apache.spark._
import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx.GraphLoader
import scala.util.MurmurHash
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.VertexId

/** Computes an approximation to pi */
object SparkPiNew2 {
  Logger.getLogger("org").setLevel(Level.OFF)
  Logger.getLogger("akka").setLevel(Level.OFF)
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Graphx Test").setMaster("local")
    val sc = new SparkContext(conf)

    
    var a:Array[Long] = new Array[Long](3)
    var z = new Array[Int](3)
    val vertices= new Array[(Long, Int)](3)
    var count = 0
          for( count <- 0 to (vertices.length-1)){
         vertices(count)= (a(count),z(count))
         println(vertices(count))
          }
    
    
    
    
//val vertices=Array((1L, ("SFO")),(2L, ("ORD")),(3L,("DFW")))
//val vRDD= sc.parallelize(vertices)
//val edges = Array(Edge(1L,2L,1800),Edge(2L,3L,800),Edge(3L,1L,1400))
//val eRDD= sc.parallelize(edges)
//// Array(Edge(1,2,1800), Edge(2,3,800))
//val nowhere = ("nowhere")
//val graph = Graph(vRDD,eRDD, nowhere)
//graph.vertices.collect.foreach(println)
}
}