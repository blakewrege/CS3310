import scala.math.random
import org.apache.spark._
import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx.GraphLoader
import scala.util.MurmurHash
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.VertexId

/** Computes an approximation to pi */
object SparkPiNew {
  Logger.getLogger("org").setLevel(Level.OFF)
  Logger.getLogger("akka").setLevel(Level.OFF)
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Graphx Test").setMaster("local")
    val sc = new SparkContext(conf)
    val logFile = "/home/blake/github/cluster-computing/demos/graph-generator/test2"
    val logData = sc.textFile(logFile, 2).cache()
    val header = logData.first().split(",")
    
    println(header(0)+ " "+header(1))

        val numNodes = header(0).toInt
        val numEdges = header(1).toInt
        val vertexArray= new Array[(Long, Int)](numNodes)
        var edgeArray= new Array[(Long, Long, Int)](numEdges)
        var count = 0
        

              for( count <- 0 to (numNodes-1)){
             vertexArray(count)= (count.toLong,count)
             println(vertexArray(count))
              }
    //    
    //             for( count <- 0 to (edgeArray.length-1)){
    //         vertexArray(count)= (a(count),z(count))
    //         println(vertexArray(count))
    //          } 
    //
    //    val vertexRDD: RDD[(Long, Int)] = sc.parallelize(vertexArray)
    //    val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)
    //
    //    val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)
    //
    //    graph.vertices.filter {
    //      case (id, (name, age)) => age > 30
    //    }.collect.foreach {
    //      case (id, (name, age)) => println(s"$name is $age")
    //    }

    //    val logFile = "/usr/local/spark/README.md" // Should be some file on your system
    //    val conf = new SparkConf().setAppName("Graphx Test")
    //    val sc = new SparkContext(conf)
    //    val logData = sc.textFile(logFile, 2).cache()
    //    val numAs = logData.filter(line => line.contains("a")).count()
    //    val numBs = logData.filter(line => line.contains("b")).count()
    //    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }

  def readCSV(fileName: String, sc: SparkContext): Array[Array[Double]] = {

    val logData = sc.textFile(fileName, 2).cache()
    var matrix: Array[Array[Double]] = Array.empty

    val rrdarr = logData.take(logData.count.toInt)
    rrdarr.foreach { line =>
      val cols = line.split(",").map(_.trim.toDouble)
      matrix = matrix :+ cols
    }
    return matrix
  }
}