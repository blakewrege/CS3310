import scala.math.random
import org.apache.spark._
import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx.GraphLoader
import scala.util.MurmurHash
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.VertexId

/** Computes an approximation to pi */
object SparkPiNew2 {
  Logger.getLogger("org").setLevel(Level.OFF)
  Logger.getLogger("akka").setLevel(Level.OFF)
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Graphx Test").setMaster("local")
    val sc = new SparkContext(conf)

    var numNodes = 3
    var numEdges = 3
    var a:Array[Long] = new Array[Long](numNodes)
    var z = new Array[Int](numNodes)
    val vertexArray= new Array[(Long, Int)](numNodes)
    var edgeArray= new Array[(Long, Long, Int)](numEdges)
    var count = 0
    a(0) = 1; a(1) = 2; a(2) = 3
    z(0) = 10; z(1) = 20; z(2) = 30
          for( count <- 0 to (vertexArray.length-1)){
         vertexArray(count)= (a(count),z(count))
         println(vertexArray(count))
          }
 val vRDD= sc.parallelize(vertexArray)
 
    
    
    
//val vertices=Array((1L, ("SFO")),(2L, ("ORD")),(3L,("DFW")))
//val vRDD= sc.parallelize(vertices)
//val edges = Array(Edge(1L,2L,1800),Edge(2L,3L,800),Edge(3L,1L,1400))
//val eRDD= sc.parallelize(edges)
//// Array(Edge(1,2,1800), Edge(2,3,800))
//val nowhere = ("nowhere")
//val graph = Graph(vRDD,eRDD, nowhere)
//graph.vertices.collect.foreach(println)
}
}