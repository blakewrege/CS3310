import scala.math.random
import org.apache.spark._
import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx.GraphLoader
import scala.util.MurmurHash
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.VertexId
import org.apache.spark.graphx.util.GraphGenerators

/** Computes an approximation to pi */
object SparkPiNew {
  Logger.getLogger("org").setLevel(Level.OFF)
  Logger.getLogger("akka").setLevel(Level.OFF)
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Parallel Prims").setMaster("local")
    val sc = new SparkContext(conf)
    
    
        // A graph with edge attributes containing distances
    val graph: Graph[Int, Double] =
      GraphGenerators.logNormalGraph(sc, numVertices = 5).mapEdges(e => e.attr.toDouble)
    graph.edges.foreach(println)
    val sourceId: VertexId = 0 // The ultimate source

    
    
    
    
    
    
//    val logFile = "/home/blake/github/cluster-computing/demos/graph-generator/test2"
//    val logData = sc.textFile(logFile, 2).cache()
//
//    val headerAndRows = logData.map(line => line.split(",").map(_.trim))
//    val header = headerAndRows.first
//    val data = headerAndRows.filter(_(0) != header(0))
//    // splits to map (header/value pairs)
//    val maps = data.map(splits => header.zip(splits).toMap)
//
//    println(header(0) + " " + header(1))
//    val numNodes = header(0).toInt
//    val numEdges = header(1).toInt
//
//    val vertexArray = new Array[(Long, String)](numNodes)
//    val edgeArray = new Array[Edge[Int]](numEdges)
//    //  var edgeArray: Array[Edge[Int]] = Array.empty
//    var count = 0
//    for (count <- 0 to numNodes - 1) {
//      vertexArray(count) = (count.toLong + 1, (count + 1).toString())
//      println(vertexArray(count))
//    }
//    count = 0
//    val rrdarr = data.take(data.count.toInt)
//    for (count <- 0 to (numEdges - 1)) {
//      val line = rrdarr(count)
//      val cols = line.toList
//      val edge = Edge(cols(0).toLong, cols(1).toLong, cols(2).toInt)
//      edgeArray(count) = Edge(cols(0).toLong, cols(1).toLong, cols(2).toInt)
//    }
//
//    edgeArray.foreach { line =>
//      println(line.toString())
//    }
//
//        val vertexRDD: RDD[(Long, (String))] = sc.parallelize(vertexArray)
//        val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)
//    
//          val graph: Graph[String,Int] = Graph(vertexRDD, edgeRDD)
//    
//graph.triplets.take(6).foreach(println)
        

    //    val logFile = "/usr/local/spark/README.md" // Should be some file on your system
    //    val conf = new SparkConf().setAppName("Graphx Test")
    //    val sc = new SparkContext(conf)
    //    val logData = sc.textFile(logFile, 2).cache()
    //    val numAs = logData.filter(line => line.contains("a")).count()
    //    val numBs = logData.filter(line => line.contains("b")).count()
    //    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }


}