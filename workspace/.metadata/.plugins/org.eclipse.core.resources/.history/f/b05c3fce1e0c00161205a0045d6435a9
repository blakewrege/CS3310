import org.apache.spark._
import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx.util._
import scala.collection.mutable.ListBuffer
import scala.io.Source

class Node(id: Int) {
  var edges = List[Edge]()

  def addEdge(other: Node, weight: Int) = {
    val edge = new Edge(this, other, weight)
    edges = edge :: edges
    edge
  }

  override def toString() = id.toString
}

class Edge(val from: Node, val to: Node, val weight: Int) extends Ordered[Edge] {
  // Inverse ordering; should really be external.
  def compare(that: Edge) = that.weight compare weight
  override def toString() = from + "," + to + "," + weight
}

object primsAlgo {
  Logger.getLogger("org").setLevel(Level.OFF)
  Logger.getLogger("akka").setLevel(Level.OFF)
  
  var numNodes = 0
  var numEdges = 0
  
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Parallel Prims").setMaster("local")
    val sc = new SparkContext(conf)
    val logFile = "NodeData.txt"
    var count = 0
    //val rowArray = readCSV("NodeData.txt",sc)
    //rowArray = .map(line => line.split(","))
    val logData = sc.textFile(logFile, 6).cache()
    val headerAndRows = logData.map(line => line.split(",").map(_.trim))
    val header = headerAndRows.first
    val rowArray = headerAndRows.filter(_(0) != header(0)).collect()
    
    
    
    val colArray = rowArray.transpose

    var nodeStruct = new Array[Node](numNodes+ 1)

    while (count < numNodes+ 1) {

      nodeStruct(count) = new Node(count)
      count = count + 1
    }
    count = 0

    while (count < colArray(0).length) {
      nodeStruct(rowArray(count)(0).toInt).addEdge(nodeStruct(rowArray(count)(1).toInt), rowArray(count)(2).toInt)
      nodeStruct(rowArray(count)(1).toInt).addEdge(nodeStruct(rowArray(count)(0).toInt), rowArray(count)(2).toInt)
      count = count + 1
    }

    count = 0

    val graph = nodeStruct.toList
    

    // generateMST(graph).sortWith(_ < _).foreach(println)
    val readLines = generateMST(graph).sortWith(_ < _).toList
    
     //   val accum = sc.accumulator(0, "My Accumulator")
    //val distData = sc.parallelize(graph).foreach { x => println(x) }
    var totalLength = 0

    while (count < readLines.length) {
      val myLine = readLines(count).toString().split(",")
      totalLength = totalLength + myLine.last.toInt
      println(myLine(0) + " <--> " + myLine(1) + "    (" + myLine(2) + ")")
      count = count + 1
    }
    println(totalLength)
    var a = 0
    for (a <- 1 to (numNodes- 1)) {
      println(nodeStruct(a).edges.toList)
    }

  }

  def readCSV(fileName: String, sc: SparkContext): Array[Array[Double]] = {
    val logData = sc.textFile(fileName, 6).cache()
    var matrix: Array[Array[Double]] = Array.empty

    val rrdarr = logData.take(logData.count.toInt)
    rrdarr.foreach { line =>
      val cols = line.split(",").map(_.trim.toDouble)
      if (cols.length ==3){
      matrix = matrix :+ cols
      }else{
       val headNode = line.split(",").toList
        numNodes = headNode(0).toInt
        numEdges = headNode(1).toInt
        
      }
      }
    return matrix
  }

  def generateMST(graph: List[Node]): List[Edge] = {
    import scala.util.Random

    val startNode = graph(Random.nextInt(graph.length))

    var mst_nodes = List(startNode)
    var mst_edges = List[Edge]()

    val pq = new scala.collection.mutable.PriorityQueue[Edge]

    startNode.edges.foreach(e => pq.enqueue(e))

    while (!pq.isEmpty) {
      val edge = pq.dequeue

      if (!(mst_nodes contains edge.to)) {
        mst_nodes = edge.to :: mst_nodes
        mst_edges = edge :: mst_edges

        edge.to.edges.foreach(e => pq.enqueue(e))
      }
    }

    mst_edges

  }
}